{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rwV9QH_S29Y"
   },
   "source": [
    "# Preprocessing articles\n",
    "* Lemmatization (nltk)\n",
    "* Stop Words (nltk)\n",
    "* Stemming (nltk)\n",
    "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estos dos comandos evitan que haya que hacer reload cada vez que se modifica un paquete\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias generales\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#Paquetes para manejo de datos\n",
    "import pandas         as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#Paquetes de nltk para preprocesamiento\n",
    "import nltk\n",
    "from   nltk.tokenize import TreebankWordTokenizer\n",
    "from   nltk.stem     import PorterStemmer, WordNetLemmatizer\n",
    "from   nltk.corpus   import stopwords\n",
    "\n",
    "#Paquetes de sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.model_selection         import cross_val_score\n",
    "from sklearn.naive_bayes             import MultinomialNB\n",
    "\n",
    "#dataset a utilizar\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain twentynewsgroup as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text target\n",
      "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...      7\n",
      "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...      4\n",
      "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...      4\n",
      "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...      1\n",
      "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...     14\n",
      "...                                                  ...    ...\n",
      "11299  From: 2120788@hydra.maths.unsw.EDU.AU ()\\nSubj...     17\n",
      "11300  From: aa888@freenet.carleton.ca (Mark Baker)\\n...     15\n",
      "11301  From: zmed16@trc.amoco.com (Michael)\\nSubject:...      6\n",
      "11302  From: rdippold@qualcomm.com (Ron \"Asbestos\" Di...     11\n",
      "11303  From: bchuang@css.itd.umich.edu (Ben Chuang)\\n...      4\n",
      "\n",
      "[11304 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def twenty_newsgroup_to_csv():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "\n",
    "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "    df.columns = ['text', 'target']\n",
    "    print(df.head(-10))\n",
    "    df.to_csv('20_newsgroup.csv')\n",
    "    \n",
    "twenty_newsgroup_to_csv() # las columnas del df son text y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joa-m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joa-m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joa-m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer  = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "random_seed = 0\n",
    "test_size   = 0.3\n",
    "cross_sets  = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "caching      = True\n",
    "dataset_path = '20_newsgroup.csv'\n",
    "\n",
    "def get_nltk_cache_path(hp):\n",
    "    cache_path = f'cache-{hp[\"isalpha\"]}'\n",
    "    return cache_path\n",
    "\n",
    "def get_sklearn_cache_path(hp):\n",
    "    cache_path = f'cache-{hp[\"isalpha\"]}-{hp[\"tf_idf\"]}-{hp[\"min_df\"]}-{hp[\"max_df\"]}'\n",
    "    return cache_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_specs = {\n",
    "    'isalpha': [True, False],\n",
    "    'tf_idf':  [True, False],\n",
    "    'min_df':  [0.01, 0.05, 0.1, 0.49],\n",
    "    'max_df':  [0.5, 0.75, 0.99],\n",
    "    'alpha':   [0.01, 0.1, 1.0, 10.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     isalpha  alpha  min_df  max_df  tf_idf\n",
      "0       True   0.01    0.01    0.50    True\n",
      "1       True   0.10    0.01    0.50    True\n",
      "2       True   1.00    0.01    0.50    True\n",
      "3       True  10.00    0.01    0.50    True\n",
      "4       True   0.01    0.01    0.75    True\n",
      "..       ...    ...     ...     ...     ...\n",
      "182    False   1.00    0.49    0.50   False\n",
      "183    False  10.00    0.49    0.50   False\n",
      "184    False   0.01    0.49    0.75   False\n",
      "185    False   0.10    0.49    0.75   False\n",
      "186    False   1.00    0.49    0.75   False\n",
      "\n",
      "[187 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Guardaremos todo en un dataFrame de Pandas\n",
    "hyperparameters = pd.DataFrame()\n",
    "\n",
    "for isalpha in hyperparameters_specs['isalpha']:\n",
    "    for tf_idf in hyperparameters_specs['tf_idf']:\n",
    "        for min_df in hyperparameters_specs['min_df']:\n",
    "            for max_df in hyperparameters_specs['max_df']:\n",
    "                for alpha in hyperparameters_specs['alpha']:\n",
    "                    hp = {\n",
    "                        'isalpha': isalpha,\n",
    "                        'alpha':   alpha,\n",
    "                        'min_df':  min_df,\n",
    "                        'max_df':  max_df,\n",
    "                        'tf_idf':  tf_idf,\n",
    "                    }\n",
    "                    hp_pandas = pd.DataFrame(hp, index=[0])\n",
    "                    hyperparameters = hyperparameters.append(hp_pandas,ignore_index=True)\n",
    "\n",
    "#Veamos como quedo\n",
    "print(hyperparameters.head(-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback para el procesamiento paralelo de Dask\n",
    "def nltk_preprocessor_callback(**kwargs):\n",
    "    #Preprocesamiento con NLTK igual que en la clase anterior\n",
    "    def preprocessor(datapoint):\n",
    "        raw_datapoint          = datapoint\n",
    "        tokenized_datapoint    = tokenizer.tokenize(raw_datapoint)\n",
    "        lemmatized_datapoint   = [lemmatizer.lemmatize(x,pos='v') for x in tokenized_datapoint]\n",
    "        nonstop_datapoint      = [x for x in lemmatized_datapoint if x not in stopwords.words('english')]\n",
    "        stemmed_datapoint      = [stemmer.stem(x) for x in nonstop_datapoint]\n",
    "        filtered_datapoint     = stemmed_datapoint\n",
    "        \n",
    "        #Salteamos esto dependiendo del hiperpar치metro isalpha\n",
    "        if kwargs.setdefault('isalpha', True):\n",
    "            alphanumeric_datapoint = [x for x in stemmed_datapoint if x.isalpha()]\n",
    "            filtered_datapoint     = alphanumeric_datapoint\n",
    "        \n",
    "        return ' '.join(filtered_datapoint)\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "def run_nltk_preprocessor(hp, dataset=None):\n",
    "    print('NLTK Preprocessing...')\n",
    "    to = time.time()\n",
    "    cache_path = get_nltk_cache_path(hp)\n",
    "    \n",
    "    #Checkeamos si ya se corri칩 el preprocesamiento para esta combinaci칩n de hiperpar치metros\n",
    "    if not (os.path.exists(cache_path) and os.path.isfile(cache_path)):\n",
    "        print('Cache miss: ', cache_path)\n",
    "\n",
    "        #Leemos el dataset\n",
    "        if caching is True:\n",
    "            dataset = pd.read_csv(dataset_path)\n",
    "        else:\n",
    "            dataset = dataset.copy()\n",
    "        preprocessor    = nltk_preprocessor_callback(isalpha=hp['isalpha'])\n",
    "        ddataset        = dd.from_pandas(dataset, npartitions=os.cpu_count())\n",
    "        dataset['text'] = ddataset['text'].map_partitions(lambda df: df.apply(preprocessor)). compute(scheduler='multiprocessing')\n",
    "        \n",
    "        #Guardamos en la cache este intento\n",
    "        if caching is True:\n",
    "            cache_path = get_nltk_cache_path(hp)\n",
    "            with open(cache_path, 'wb') as fp:\n",
    "                pickle.dump(dataset, fp)\n",
    "        \n",
    "    tf = time.time()\n",
    "    print('finished in', (int(tf-to)), 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Preprocessing...\n",
      "Cache miss:  cache-True\n",
      "finished in 710 seconds.\n"
     ]
    }
   ],
   "source": [
    "for idx,hyperParam in hyperparameters.iterrows():\n",
    "    break\n",
    "run_nltk_preprocessor(hyperParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#186*710 = segs totales = 2200 min = 36.68 horas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento: sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sklearn_preprocessor(hp, dataset=None):\n",
    "    print('sklearn preprocessing...')\n",
    "    to = time.time()\n",
    "    cache_path = get_sklearn_cache_path(hp)\n",
    "    \n",
    "    #Checkeamos si ya intentamos con esta combinaci칩n\n",
    "    if not (os.path.exists(cache_path) and os.path.isfile(cache_path)):    \n",
    "        print('Cache miss: ', cache_path)   \n",
    "        \n",
    "        if caching is True:\n",
    "            cache_path = get_nltk_cache_path(hp)\n",
    "            with open (cache_path, 'rb') as fp:\n",
    "                dataset = pickle.load(fp)\n",
    "        else:\n",
    "            dataset = dataset.copy()\n",
    "\n",
    "        #Corremos el vectorizer que corresponde, igual que en clase anterior\n",
    "        V = (TfidfVectorizer if hp['tf_idf'] is True else CountVectorizer)(min_df=hp['min_df'], max_df=hp['max_df'])\n",
    "        X = V.fit_transform(dataset['text']).toarray()\n",
    "        Y = np.array([dataset['target'].values]).T\n",
    "        D = np.hstack((X, Y))\n",
    "\n",
    "        np.random.seed(seed=random_seed)\n",
    "        np.random.shuffle(D)\n",
    "\n",
    "        if caching is True:\n",
    "            cache_path = get_sklearn_cache_path(hp)\n",
    "            with open(cache_path, 'wb') as fp:\n",
    "                pickle.dump(D, fp)\n",
    "\n",
    "    tf = time.time()\n",
    "    print('finished in', (int(tf-to)), 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,hp2 in hyperparameters.iterrows():\n",
    "    break\n",
    "run_sklearn_preprocessor(hp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isalpha': True, 'alpha': 0.01, 'min_df': 0.01, 'max_df': 0.5, 'tf_idf': True}\n",
      "NLTK Preprocessing...\n",
      "Cache miss:  cache-True\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File emails.csv does not exist: 'emails.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a3ce0c1f93cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrun_nltk_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mrun_sklearn_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-9e8e85e8da83>\u001b[0m in \u001b[0;36mrun_nltk_preprocessor\u001b[1;34m(hp, dataset)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#Leemos el dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcaching\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\redes2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\redes2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\redes2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\redes2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\redes2\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File emails.csv does not exist: 'emails.csv'"
     ]
    }
   ],
   "source": [
    "print('Preprocessing dataset...')\n",
    "for index, hp in hyperparameters.iterrows():\n",
    "    print(hp.to_dict())\n",
    "    run_nltk_preprocessor(hp)\n",
    "    run_sklearn_preprocessor(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training model with best hyperparameters...')\n",
    "\n",
    "#Me quedo con la mejor combinaci칩n de hiperpar치metros.\n",
    "best_hp = scores.loc[scores['score'].idxmax()].drop(['score'])\n",
    "print(best_hp.to_dict())\n",
    "\n",
    "if caching is True:\n",
    "    cache_path = get_sklearn_cache_path(best_hp)\n",
    "    with open (cache_path, 'rb') as fp:\n",
    "        D = pickle.load(fp)\n",
    "else:\n",
    "    D = dataset.copy()\n",
    "\n",
    "X = D[:,:D.shape[1]-1]\n",
    "Y = D[:,D.shape[1]-1:].flatten()\n",
    "\n",
    "#Separamos el dataset para train y validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, shuffle=False)\n",
    "\n",
    "#Creamos el clasificador para los mejores hiperpar치metros\n",
    "clf = MultinomialNB(alpha=best_hp['alpha'], class_prior=None, fit_prior=False)\n",
    "\n",
    "#Entrenamos el modelo\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating best model...')\n",
    "    \n",
    "if caching is True:\n",
    "    cache_path = get_sklearn_cache_path(best_hp)\n",
    "    with open (cache_path, 'rb') as fp:\n",
    "        D = pickle.load(fp)\n",
    "else:\n",
    "    D = dataset.copy()\n",
    "\n",
    "X = D[:,:D.shape[1]-1]\n",
    "Y = D[:,D.shape[1]-1:].flatten()\n",
    "\n",
    "#Separo el set para train y test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, shuffle=False)\n",
    "    \n",
    "#Vemos el score final del modelo para test\n",
    "score = clf.score(X_test, Y_test)\n",
    "print(\"accuracy: {:.4}%\".format(score*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Naive Bayes para TNG.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "redes2 (py 3.6)",
   "language": "python",
   "name": "redes2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
